{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iMaW4NgYnq0e"
   },
   "source": [
    "# Basic config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J1PX4XZYkCdl"
   },
   "outputs": [],
   "source": [
    "#Uploading dataset from GoogleDrive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mW6pj4lOlNEP"
   },
   "outputs": [],
   "source": [
    "#To go to your driveâ€™s main directory\n",
    "cd /content/gdrive/My Drive/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9k5FkYfTq9-P"
   },
   "outputs": [],
   "source": [
    "img_dir = '/content/gdrive/My Drive/dataset/'\n",
    "print(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9e2oHvmwTZ2"
   },
   "outputs": [],
   "source": [
    "cd '/content/gdrive/My Drive/dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrrCTnRTny5O"
   },
   "source": [
    "# Data preprocessing-Split folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RWPGPAoXrKDO"
   },
   "outputs": [],
   "source": [
    "!pip install split-folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ZDq2H44rTy4"
   },
   "outputs": [],
   "source": [
    "#https://github.com/jfilter/split-folders\n",
    "import split_folders\n",
    "\n",
    "# Split with a ratio.\n",
    "# To only split into training and validation set, set a tuple to `ratio`, i.e, `(.8, .2)`.\n",
    "\n",
    "#==========================UNCOMMENT FOR SPLITING=============================================\n",
    "split_folders.ratio(img_dir, output=\"output\", seed=1337, ratio=(.7, .15, .15)) # default values\n",
    "#================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C9SeZde8n4Bj"
   },
   "source": [
    "# Import libraries and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tws7ObismOZH"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras import backend as K\n",
    "\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "gpu_options = tf.GPUOptions(allow_growth=True)\n",
    "session = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NmDLUt2aiqpL"
   },
   "outputs": [],
   "source": [
    "#Prepare Data\n",
    "img_width, img_height = 200, 200\n",
    "\n",
    "output_dir = img_dir + 'output/'\n",
    "\n",
    "train_data_dir = os.path.join(output_dir, 'train')\n",
    "print(train_data_dir)\n",
    "validation_data_dir = os.path.join(output_dir, 'val')\n",
    "test_data_dir = os.path.join(output_dir, 'test')\n",
    "nb_train_samples = 1274  \n",
    "nb_validation_samples = 275  \n",
    "nb_test_samples = 281  \n",
    "epochs = 4\n",
    "batch_size = 16\n",
    "numclasses = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iIQCtxdRoTBM"
   },
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7bP1mbMpivk7"
   },
   "outputs": [],
   "source": [
    "# dataset\n",
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "    zoom_range = 0.1, # Randomly zoom image \n",
    "    width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "    height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "    #shear_range=0.2,\n",
    "    vertical_flip=False,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYBRX3JliX7f"
   },
   "outputs": [],
   "source": [
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h4UBlGlqoWpT"
   },
   "source": [
    "# Create the base model from the pre-trained convnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_O2TAoXci5nT"
   },
   "outputs": [],
   "source": [
    "#Model\n",
    "#if channel is first set input shape => (3, 200, 200) else (200, 200, 3)\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Py4EluM_nagJ"
   },
   "outputs": [],
   "source": [
    "base_model = None\n",
    "base_model = keras.applications.resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IASe3qP-p0bM"
   },
   "source": [
    "**Freeze Convolutional base**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4EFViwKnpxu"
   },
   "outputs": [],
   "source": [
    "# It is important to freeze the convolutional base before you compile and train the model. \n",
    "#Freezing (by setting layer.trainable = False) prevents the weights in a given layer from being updated during training. \n",
    "#MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozYjqNAunnMm"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RlCb7GQujd2c"
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oVkYLmKmp-st"
   },
   "outputs": [],
   "source": [
    "print(base_model.output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xaF7d-MenuSn"
   },
   "outputs": [],
   "source": [
    "top_model = Sequential()\n",
    "top_model.add(Flatten(input_shape=base_model.output_shape[1:])) #Flattens the input. Does not affect the batch size.\n",
    "for i in range(2):\n",
    "  top_model.add(Dense(4096, activation='relu')) #Apply a Dense layer to convert these features into a single prediction per image. \n",
    "  top_model.add(Dropout(0.5)) #Dropout consists in randomly setting a fraction rate of input units to 0.5 at each update during training time, which helps prevent overfitting.\n",
    "top_model.add(Dense(numclasses, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nF1gEM20L8hw"
   },
   "outputs": [],
   "source": [
    "top_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Xnry55Eofqy"
   },
   "outputs": [],
   "source": [
    "#https://keras.io/getting-started/functional-api-guide/\n",
    "model = None\n",
    "model = Model(inputs=base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mlGszOiuo1Hb"
   },
   "source": [
    "**Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbBRJKcWjSnb"
   },
   "outputs": [],
   "source": [
    "#model = resnet50tl(input_shape, numclasses, 'softmax')\n",
    "lr = 1e-5\n",
    "decay = 1e-7 #0.0\n",
    "optimizer = RMSprop(lr=lr, decay=decay)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rp97U2IasPvD"
   },
   "source": [
    "# Display model/network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IC8Z6i8i_ffg"
   },
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "#plot_model(model, show_shapes=True, show_layer_names = True)\n",
    "from IPython.display import Image\n",
    "\n",
    "#Uncomment for visualisation\n",
    "\n",
    "#Image(filename='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ozIBJ2mocTj"
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04iFdQlhjUDq"
   },
   "outputs": [],
   "source": [
    "#Train\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UBBkEDsuokND"
   },
   "source": [
    "# Performance and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWrgtNFCjWVK"
   },
   "outputs": [],
   "source": [
    "# Get training and test loss histories - Learning curves\n",
    "training_loss = history.history['loss']\n",
    "training_acc = history.history['acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "# Visualize loss history\n",
    "fig.add_subplot(121)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, training_acc, 'b-')\n",
    "plt.legend(['Training Loss', 'Training Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss/Acc')\n",
    "\n",
    "# Get training and test loss histories\n",
    "val_acc = history.history['val_acc']\n",
    "training_acc = history.history['acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(val_acc) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "fig.add_subplot(122)\n",
    "plt.plot(epoch_count, val_acc, 'r--')\n",
    "plt.plot(epoch_count, training_acc, 'b-')\n",
    "plt.legend(['Validation Accuracy', 'Training Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDdq-lzau4i9"
   },
   "outputs": [],
   "source": [
    "saveweight =  'celebriytag_weight_transfer.h5'\n",
    "model.save_weights(saveweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6N8aYS6T3GHx"
   },
   "outputs": [],
   "source": [
    "model.load_weights(saveweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8G1CXx3P2fAH"
   },
   "outputs": [],
   "source": [
    "#Evaluate the model on test set\n",
    "print(model.evaluate_generator(test_generator,steps= (nb_test_samples // batch_size), verbose = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "goanDWRevdNJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "\n",
    "test_generator.reset()\n",
    "\n",
    "predictions = model.predict_generator(test_generator)\n",
    "\n",
    "Y_pred = model.predict_generator(test_generator)\n",
    "classes = test_generator.classes[test_generator.index_array]\n",
    "y_pred = np.argmax(Y_pred, axis=-1)\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(test_generator.classes[test_generator.index_array],y_pred))\n",
    "\n",
    "\n",
    "predicted_classes = numpy.argmax(predictions, axis=1)\n",
    "\n",
    "report = metrics.classification_report(classes, predicted_classes, target_names=class_labels)\n",
    "print('\\nClassification Report: \\n')\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AW9g5HAZooew"
   },
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LKQ2_eVosZ1r"
   },
   "outputs": [],
   "source": [
    "#UnFreeze all the layers of base_model to train it after 5 epochs \n",
    "base_model.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWdrME_FsccC"
   },
   "outputs": [],
   "source": [
    "# Let's take a look to see how many layers are in the base model\n",
    "print(\"Number of layers in the base model: \", len(base_model.layers))\n",
    "\n",
    "\n",
    "# # Freeze all the layers before the `fine_tune_at` layer\n",
    "# for layer in base_model.layers[:fine_tune_at]:\n",
    "#   layer.trainable =  False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qCYvCTP6tKbi"
   },
   "outputs": [],
   "source": [
    "model2 = None\n",
    "model2 = Model(inputs=base_model.input, outputs=top_model(base_model.output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Ilc2eOTqGIj"
   },
   "source": [
    "**Compile the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyaMzgcytKrw"
   },
   "outputs": [],
   "source": [
    "lr = 1e-5\n",
    "decay = 1e-7 #0.0\n",
    "optimizer = RMSprop(lr=lr, decay=decay)\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iq56KW5btK2K"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apNCagO_rk00"
   },
   "source": [
    "# Continue training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UtBq2H9K-EZR"
   },
   "outputs": [],
   "source": [
    "#Train no2\n",
    "fine_tune_epochs = 12\n",
    "total_epochs =  epochs + fine_tune_epochs\n",
    "\n",
    "history = model2.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=total_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILfNE2kwr1Tc"
   },
   "source": [
    "# Performance and Evaluation after fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dctXnD8atnc5"
   },
   "outputs": [],
   "source": [
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "training_acc = history.history['acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "# Visualize loss history\n",
    "fig.add_subplot(121)\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, training_acc, 'b-')\n",
    "plt.legend(['Training Loss', 'Training Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss/Acc')\n",
    "\n",
    "# Get training and test loss histories\n",
    "val_acc = history.history['val_acc']\n",
    "training_acc = history.history['acc']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(val_acc) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "fig.add_subplot(122)\n",
    "plt.plot(epoch_count, val_acc, 'r--')\n",
    "plt.plot(epoch_count, training_acc, 'b-')\n",
    "plt.legend(['Validation Accuracy', 'Training Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCmZb763Jzln"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "\n",
    "test_generator.reset()\n",
    "\n",
    "predictions2 = model2.predict_generator(test_generator)\n",
    "\n",
    "Y_pred2 = model2.predict_generator(test_generator)\n",
    "classes = test_generator.classes[test_generator.index_array]\n",
    "y_pred2 = np.argmax(Y_pred2, axis=-1)\n",
    "\n",
    "predicted_classes2 = numpy.argmax(predictions2, axis=1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion matrix: \\n')\n",
    "print(confusion_matrix(test_generator.classes[test_generator.index_array],y_pred2))\n",
    "\n",
    "report2 = metrics.classification_report(classes, predicted_classes2, target_names=class_labels)\n",
    "print('\\nClassification Report: \\n')\n",
    "print(report2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9Dtj9ciztnZI"
   },
   "outputs": [],
   "source": [
    "saveweight2 =  'celebriytag_weight_transfer2.h5'\n",
    "model2.save_weights(saveweight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MWTJSWPbtnQK"
   },
   "outputs": [],
   "source": [
    "model2.load_weights(saveweight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qF7zICvrtnMR"
   },
   "outputs": [],
   "source": [
    "#Evaluate the model on test set\n",
    "print(model2.evaluate_generator(test_generator,steps= (nb_test_samples // batch_size), verbose = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sFDCGjTOsGlk"
   },
   "source": [
    "# Evaluation per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "25Wmaa2Ru95F"
   },
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import cv2\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eHWcDeh_vxo1"
   },
   "outputs": [],
   "source": [
    "labels = ['aaron_diaz',  'aaron_tippin',  'aarti_chabria',  'abbey_clancy',  'abby_elliott', 'uknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ET_kmuI55IxF"
   },
   "outputs": [],
   "source": [
    "test_imgs = ['0311_01.jpg']\n",
    "\n",
    "new_validation_data_dir = '/content/gdrive/My Drive/dataset/output/test/aaron_tippin/'\n",
    "\n",
    "print(new_validation_data_dir)\n",
    "\n",
    "for test in test_imgs:\n",
    "    test_img = os.path.join(new_validation_data_dir, test)\n",
    "    img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.\n",
    "    classes = model.predict(x)\n",
    "    result = np.squeeze(classes)\n",
    "    result_indices = np.argmax(result)\n",
    "    \n",
    "    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9xZ5tQgwmH-"
   },
   "outputs": [],
   "source": [
    "test_imgs = ['0058_01.jpg']\n",
    "\n",
    "new_validation_data_dir = '/content/gdrive/My Drive/dataset/output/test/abbey_clancy/'\n",
    "\n",
    "for test in test_imgs:\n",
    "    test_img = os.path.join(new_validation_data_dir, test)\n",
    "    img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.\n",
    "    classes = model.predict(x)\n",
    "    result = np.squeeze(classes)\n",
    "    result_indices = np.argmax(result)\n",
    "    \n",
    "    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RMIlftPbwvY"
   },
   "outputs": [],
   "source": [
    "test_imgs = ['0081_01.jpg']\n",
    "\n",
    "new_validation_data_dir = '/content/gdrive/My Drive/dataset/output/test/aaron_diaz/'\n",
    "\n",
    "for test in test_imgs:\n",
    "    test_img = os.path.join(new_validation_data_dir, test)\n",
    "    img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.\n",
    "    classes = model.predict(x)\n",
    "    result = np.squeeze(classes)\n",
    "    result_indices = np.argmax(result)\n",
    "    \n",
    "    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TbQJxUwzp3ak"
   },
   "outputs": [],
   "source": [
    "test_imgs = ['0205_02.jpg']\n",
    "\n",
    "new_validation_data_dir = '/content/gdrive/My Drive/dataset/output/test/aarti_chabria/'\n",
    "\n",
    "for test in test_imgs:\n",
    "    test_img = os.path.join(new_validation_data_dir, test)\n",
    "    img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.\n",
    "    classes = model.predict(x)\n",
    "    result = np.squeeze(classes)\n",
    "    result_indices = np.argmax(result)\n",
    "    \n",
    "    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NsbYSsadfGKp"
   },
   "outputs": [],
   "source": [
    "test_imgs = ['0037_01.jpg']\n",
    "\n",
    "new_validation_data_dir = '/content/gdrive/My Drive/dataset/output/test/abby_elliott/'\n",
    "\n",
    "for test in test_imgs:\n",
    "    test_img = os.path.join(new_validation_data_dir, test)\n",
    "    img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.\n",
    "    classes = model.predict(x)\n",
    "    result = np.squeeze(classes)\n",
    "    result_indices = np.argmax(result)\n",
    "    \n",
    "    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oaDxn0DufSxN"
   },
   "outputs": [],
   "source": [
    "test_imgs = ['0003_01.jpg']\n",
    "\n",
    "new_validation_data_dir = '/content/gdrive/My Drive/dataset/output/test/unkown/'\n",
    "\n",
    "for test in test_imgs:\n",
    "    test_img = os.path.join(new_validation_data_dir, test)\n",
    "    img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.\n",
    "    classes = model.predict(x)\n",
    "    result = np.squeeze(classes)\n",
    "    result_indices = np.argmax(result)\n",
    "    \n",
    "    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jbjbzoesfbl4"
   },
   "outputs": [],
   "source": [
    "test_imgs = ['0051_02 (2).jpg']\n",
    "\n",
    "new_validation_data_dir = '/content/gdrive/My Drive/dataset/output/test/unkown/'\n",
    "\n",
    "for test in test_imgs:\n",
    "    test_img = os.path.join(new_validation_data_dir, test)\n",
    "    img = image.load_img(test_img, target_size=(img_width, img_height))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 255.\n",
    "    classes = model.predict(x)\n",
    "    result = np.squeeze(classes)\n",
    "    result_indices = np.argmax(result)\n",
    "    \n",
    "    img = cv2.imread(test_img, cv2.IMREAD_COLOR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"{}, {:.2f}%\".format(labels[result_indices], result[result_indices]*100))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhRJ7mu857wc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iMaW4NgYnq0e",
    "hrrCTnRTny5O",
    "C9SeZde8n4Bj",
    "iIQCtxdRoTBM",
    "h4UBlGlqoWpT",
    "rp97U2IasPvD",
    "8ozIBJ2mocTj",
    "UBBkEDsuokND",
    "AW9g5HAZooew",
    "apNCagO_rk00",
    "ILfNE2kwr1Tc",
    "sFDCGjTOsGlk"
   ],
   "name": "part_3(celebrity_classification)-final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
